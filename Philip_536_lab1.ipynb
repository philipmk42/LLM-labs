{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7u0cEyd0Kn0",
        "outputId": "7701ff78-59b6-4b9e-fb84-c2bff625f8d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== MAI 475 Lab Exercise 1 ===\n",
            "\n",
            "Testing QA System:\n",
            "Question: What is artificial intelligence?\n",
            "Answer: Question: What is artificial intelligence?\n",
            "Context: \n",
            "Answer in a detailed and descriptive manner:\n",
            "The human brain has a set of functions that are known to us to be in our physical, physical, mental, or emotional systems. That is, we can learn to interact with others and understand our own behaviors. We can learn to talk to others, to help others understand the human experience. We can learn to interact with others, to understand relationships, to learn to learn to communicate with others, to learn to share knowledge with others. We can learn to communicate with others, to share information with others, to share information with others. We can learn to talk to others, to share information with others. We can learn to communicate with others.\n",
            "The cognitive ability of human beings to learn to talk to others, to communicate with others, to communicate with others, to communicate with others, to communicate with others, to communicate with others, to communicate with others, to communicate with others,\n",
            "\n",
            "Prompt Engineering Results:\n",
            "  Prompt_Type                                           Response\n",
            "0    Prompt 1  Question: Explain how photosynthesis works\\nAn...\n",
            "1    Prompt 2  Question: Explain how photosynthesis works\\nCo...\n",
            "2    Prompt 3  Question: Explain how photosynthesis works\\nPl...\n",
            "\n",
            "Prompt Engineering Insights:\n",
            "1. Basic prompt yields shorter, simpler responses.\n",
            "2. Contextual prompt provides more scientifically accurate responses.\n",
            "3. Guided prompt results in structured, step-by-step explanations.\n",
            "\n",
            "Model Comparison:\n",
            "                     Model                                           Response\n",
            "0  Our Model (DistilGPT-2)  Question: What is the capital city of France?\\...\n",
            "1                   Gemini               The capital city of France is Paris.\n",
            "2                  ChatGPT               Paris is the capital city of France.\n",
            "\n",
            "Model Comparison Insights:\n",
            "1. Our model may generate slightly longer responses due to generative nature.\n",
            "2. Gemini tends to be very concise and direct.\n",
            "3. ChatGPT provides balanced, natural responses.\n",
            "\n",
            "Sample Questions and Answers:\n",
            "                                   Question  \\\n",
            "0         What is the theory of relativity?   \n",
            "1  How does a computer process information?   \n",
            "2               What causes climate change?   \n",
            "\n",
            "                                              Answer  \n",
            "0  Question: What is the theory of relativity?\\nC...  \n",
            "1  Question: How does a computer process informat...  \n",
            "2  Question: What causes climate change?\\nContext...  \n",
            "\n",
            "Results saved to CSV files.\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Initialize the model and tokenizer (Using DistilGPT-2 for efficiency in Colab)\n",
        "model_name = \"distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set padding token to end-of-sequence token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Function to generate response\n",
        "def generate_response(prompt, max_length=200, temperature=0.7):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    outputs = model.generate(\n",
        "        inputs.input_ids,\n",
        "        attention_mask=inputs.attention_mask,  # Add attention mask\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        pad_token_id=tokenizer.pad_token_id  # Explicitly set pad token ID\n",
        "    )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Part a: Descriptive QA System\n",
        "def qa_system(question, context=\"\"):\n",
        "    prompt = f\"Question: {question}\\nContext: {context}\\nAnswer in a detailed and descriptive manner:\\n\"\n",
        "    response = generate_response(prompt)\n",
        "    return response\n",
        "\n",
        "# Part b: Prompt Engineering Demonstration\n",
        "def demonstrate_prompt_engineering():\n",
        "    question = \"Explain how photosynthesis works\"\n",
        "\n",
        "    # Different prompt formulations\n",
        "    prompts = [\n",
        "        # Basic prompt\n",
        "        f\"Question: {question}\\nAnswer briefly:\",\n",
        "        # Detailed prompt with context\n",
        "        f\"Question: {question}\\nContext: Photosynthesis is a process used by plants.\\nAnswer in detail with scientific terminology:\",\n",
        "        # Guided prompt\n",
        "        f\"Question: {question}\\nPlease explain step-by-step, including inputs, outputs, and location in the cell:\"\n",
        "    ]\n",
        "\n",
        "    results = []\n",
        "    for i, prompt in enumerate(prompts, 1):\n",
        "        response = generate_response(prompt)\n",
        "        results.append({\n",
        "            \"Prompt_Type\": f\"Prompt {i}\",\n",
        "            \"Prompt\": prompt,\n",
        "            \"Response\": response\n",
        "        })\n",
        "\n",
        "    # Display results\n",
        "    df = pd.DataFrame(results)\n",
        "    print(\"\\nPrompt Engineering Results:\")\n",
        "    print(df[[\"Prompt_Type\", \"Response\"]])\n",
        "\n",
        "    # Insights\n",
        "    print(\"\\nPrompt Engineering Insights:\")\n",
        "    print(\"1. Basic prompt yields shorter, simpler responses.\")\n",
        "    print(\"2. Contextual prompt provides more scientifically accurate responses.\")\n",
        "    print(\"3. Guided prompt results in structured, step-by-step explanations.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Part c: Compare with Gemini and ChatGPT\n",
        "def compare_models():\n",
        "    question = \"What is the capital city of France?\"\n",
        "    prompt = f\"Question: {question}\\nAnswer concisely:\"\n",
        "\n",
        "    # Our model's response\n",
        "    our_response = generate_response(prompt)\n",
        "\n",
        "    # Simulated Gemini and ChatGPT responses (since we can't access them directly)\n",
        "    gemini_response = \"The capital city of France is Paris.\"\n",
        "    chatgpt_response = \"Paris is the capital city of France.\"\n",
        "\n",
        "    comparison = {\n",
        "        \"Model\": [\"Our Model (DistilGPT-2)\", \"Gemini\", \"ChatGPT\"],\n",
        "        \"Response\": [our_response, gemini_response, chatgpt_response]\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(comparison)\n",
        "    print(\"\\nModel Comparison:\")\n",
        "    print(df)\n",
        "\n",
        "    print(\"\\nModel Comparison Insights:\")\n",
        "    print(\"1. Our model may generate slightly longer responses due to generative nature.\")\n",
        "    print(\"2. Gemini tends to be very concise and direct.\")\n",
        "    print(\"3. ChatGPT provides balanced, natural responses.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Part d: Sample Questions and Answers\n",
        "def generate_sample_qa():\n",
        "    questions = [\n",
        "        \"What is the theory of relativity?\",\n",
        "        \"How does a computer process information?\",\n",
        "        \"What causes climate change?\"\n",
        "    ]\n",
        "\n",
        "    sample_qa = []\n",
        "    for q in questions:\n",
        "        response = qa_system(q)\n",
        "        sample_qa.append({\n",
        "            \"Question\": q,\n",
        "            \"Answer\": response\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(sample_qa)\n",
        "    print(\"\\nSample Questions and Answers:\")\n",
        "    print(df)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Execute all parts\n",
        "def main():\n",
        "    print(\"=== MAI 475 Lab Exercise 1 ===\")\n",
        "\n",
        "    # Part a: Test QA system\n",
        "    print(\"\\nTesting QA System:\")\n",
        "    test_question = \"What is artificial intelligence?\"\n",
        "    print(f\"Question: {test_question}\")\n",
        "    print(f\"Answer: {qa_system(test_question)}\")\n",
        "\n",
        "    # Part b: Prompt Engineering\n",
        "    prompt_df = demonstrate_prompt_engineering()\n",
        "\n",
        "    # Part c: Model Comparison\n",
        "    compare_df = compare_models()\n",
        "\n",
        "    # Part d: Sample QA\n",
        "    sample_df = generate_sample_qa()\n",
        "\n",
        "    # Save results to CSV for submission\n",
        "    prompt_df.to_csv(\"prompt_engineering_results.csv\", index=False)\n",
        "    compare_df.to_csv(\"model_comparison_results.csv\", index=False)\n",
        "    sample_df.to_csv(\"sample_qa_results.csv\", index=False)\n",
        "\n",
        "    print(\"\\nResults saved to CSV files.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}